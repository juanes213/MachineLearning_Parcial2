{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "jdydFbPbauMU",
      "metadata": {
        "id": "jdydFbPbauMU"
      },
      "source": [
        "# Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "_01x9GkuOe-S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_01x9GkuOe-S",
        "outputId": "d3ac07b0-65fc-4eae-fece-7b7862bfcdca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl.metadata (543 bytes)\n",
            "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bayesian-optimization) (1.23.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bayesian-optimization) (1.9.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bayesian-optimization) (1.1.3)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "  Attempting uninstall: colorama\n",
            "    Found existing installation: colorama 0.4.5\n",
            "    Uninstalling colorama-0.4.5:\n",
            "      Successfully uninstalled colorama-0.4.5\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "sfW--pU4auMX",
      "metadata": {
        "id": "sfW--pU4auMX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import timedelta\n",
        "from scipy.stats import skew, pearsonr, spearmanr\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import BayesianRidge, Ridge, Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from xgboost import XGBRegressor\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm.notebook import tqdm\n",
        "from math import sqrt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7PLW2rb5auMY",
      "metadata": {
        "id": "7PLW2rb5auMY"
      },
      "source": [
        "# Cargar los Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "m6AGjDlLauMY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "m6AGjDlLauMY",
        "outputId": "6995064c-f894-4cc4-c28b-da82bddecb8f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hora_UTC</th>\n",
              "      <th>Direccion_Viento_Grados</th>\n",
              "      <th>Velocidad_Viento_m_s</th>\n",
              "      <th>Humedad_Max_Hora_Anterior</th>\n",
              "      <th>Humedad_Min_Hora_Anterior</th>\n",
              "      <th>Temperatura_Max_Hora_Anterior_C</th>\n",
              "      <th>Temperatura_Min_Hora_Anterior_C</th>\n",
              "      <th>Humedad_Relativa_Horaria</th>\n",
              "      <th>Presion_Atmosferica_Estacion_mB</th>\n",
              "      <th>Precipitacion_Total_Horario_mm</th>\n",
              "      <th>Viento_Rajada_Maxima_m_s</th>\n",
              "      <th>Presion_Atmosferica_Max_Hora_Anterior_mB</th>\n",
              "      <th>Presion_Atmosferica_Min_Hora_Anterior_mB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12:00</td>\n",
              "      <td>0.809017</td>\n",
              "      <td>1.8</td>\n",
              "      <td>69.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>22.6</td>\n",
              "      <td>20.7</td>\n",
              "      <td>61.0</td>\n",
              "      <td>888.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>888.2</td>\n",
              "      <td>887.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13:00</td>\n",
              "      <td>0.965926</td>\n",
              "      <td>2.7</td>\n",
              "      <td>62.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>24.2</td>\n",
              "      <td>22.5</td>\n",
              "      <td>55.0</td>\n",
              "      <td>888.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>888.4</td>\n",
              "      <td>888.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14:00</td>\n",
              "      <td>0.891007</td>\n",
              "      <td>2.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>25.5</td>\n",
              "      <td>24.3</td>\n",
              "      <td>51.0</td>\n",
              "      <td>888.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.9</td>\n",
              "      <td>888.4</td>\n",
              "      <td>888.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15:00</td>\n",
              "      <td>0.848048</td>\n",
              "      <td>2.5</td>\n",
              "      <td>52.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>27.4</td>\n",
              "      <td>25.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>887.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>888.1</td>\n",
              "      <td>887.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16:00</td>\n",
              "      <td>0.224951</td>\n",
              "      <td>2.4</td>\n",
              "      <td>50.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>27.1</td>\n",
              "      <td>25.5</td>\n",
              "      <td>46.0</td>\n",
              "      <td>886.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>887.4</td>\n",
              "      <td>886.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Hora_UTC  Direccion_Viento_Grados  Velocidad_Viento_m_s  \\\n",
              "0    12:00                 0.809017                   1.8   \n",
              "1    13:00                 0.965926                   2.7   \n",
              "2    14:00                 0.891007                   2.0   \n",
              "3    15:00                 0.848048                   2.5   \n",
              "4    16:00                 0.224951                   2.4   \n",
              "\n",
              "   Humedad_Max_Hora_Anterior  Humedad_Min_Hora_Anterior  \\\n",
              "0                       69.0                       60.0   \n",
              "1                       62.0                       55.0   \n",
              "2                       56.0                       50.0   \n",
              "3                       52.0                       44.0   \n",
              "4                       50.0                       43.0   \n",
              "\n",
              "   Temperatura_Max_Hora_Anterior_C  Temperatura_Min_Hora_Anterior_C  \\\n",
              "0                             22.6                             20.7   \n",
              "1                             24.2                             22.5   \n",
              "2                             25.5                             24.3   \n",
              "3                             27.4                             25.0   \n",
              "4                             27.1                             25.5   \n",
              "\n",
              "   Humedad_Relativa_Horaria  Presion_Atmosferica_Estacion_mB  \\\n",
              "0                      61.0                            888.2   \n",
              "1                      55.0                            888.4   \n",
              "2                      51.0                            888.1   \n",
              "3                      44.0                            887.4   \n",
              "4                      46.0                            886.5   \n",
              "\n",
              "   Precipitacion_Total_Horario_mm  Viento_Rajada_Maxima_m_s  \\\n",
              "0                             0.0                       3.8   \n",
              "1                             0.0                       4.7   \n",
              "2                             0.0                       4.9   \n",
              "3                             0.0                       5.8   \n",
              "4                             0.0                       5.8   \n",
              "\n",
              "   Presion_Atmosferica_Max_Hora_Anterior_mB  \\\n",
              "0                                     888.2   \n",
              "1                                     888.4   \n",
              "2                                     888.4   \n",
              "3                                     888.1   \n",
              "4                                     887.4   \n",
              "\n",
              "   Presion_Atmosferica_Min_Hora_Anterior_mB  \n",
              "0                                     887.7  \n",
              "1                                     888.2  \n",
              "2                                     888.1  \n",
              "3                                     887.4  \n",
              "4                                     886.5  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = 'data_treino_dv_df_2000_2010_new.csv'\n",
        "df_wind = pd.read_csv(file_path)\n",
        "df_wind.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "Jk1Bc7ehauMd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk1Bc7ehauMd",
        "outputId": "856828ed-7218-40ee-fced-3a0ee71e379c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eliminando Humedad_Min_Hora_Anterior con VIF: 80.56035297652929\n",
            "Eliminando Humedad_Relativa_Horaria con VIF: 46.37960517199195\n",
            "Eliminando Temperatura_Max_Hora_Anterior_C con VIF: 22.196504568127665\n",
            "Eliminando Presion_Atmosferica_Min_Hora_Anterior_mB con VIF: 16.065510383402767\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Variable</th>\n",
              "      <th>VIF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Direccion_Viento_Grados</td>\n",
              "      <td>1.286314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Humedad_Max_Hora_Anterior</td>\n",
              "      <td>2.130640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Temperatura_Min_Hora_Anterior_C</td>\n",
              "      <td>1.978843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Presion_Atmosferica_Estacion_mB</td>\n",
              "      <td>4.547480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Precipitacion_Total_Horario_mm</td>\n",
              "      <td>1.064873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Viento_Rajada_Maxima_m_s</td>\n",
              "      <td>1.250473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Presion_Atmosferica_Max_Hora_Anterior_mB</td>\n",
              "      <td>4.719906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Variable       VIF\n",
              "1                   Direccion_Viento_Grados  1.286314\n",
              "2                 Humedad_Max_Hora_Anterior  2.130640\n",
              "3           Temperatura_Min_Hora_Anterior_C  1.978843\n",
              "4           Presion_Atmosferica_Estacion_mB  4.547480\n",
              "5            Precipitacion_Total_Horario_mm  1.064873\n",
              "6                  Viento_Rajada_Maxima_m_s  1.250473\n",
              "7  Presion_Atmosferica_Max_Hora_Anterior_mB  4.719906"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def calculate_vif(df):\n",
        "    \"\"\"\n",
        "    Calcula el VIF para cada variable en el DataFrame proporcionado.\n",
        "    \"\"\"\n",
        "    VIF = pd.DataFrame()\n",
        "    df = add_constant(df)  # Añade una constante para el cálculo del VIF\n",
        "    VIF[\"Variable\"] = df.columns\n",
        "    VIF[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "    VIF = VIF[VIF[\"Variable\"] != \"const\"]\n",
        "    return VIF\n",
        "\n",
        "df_numeric = df_wind.select_dtypes(include=['float64', 'int64']).dropna()\n",
        "\n",
        "# Eliminar la variable de respuesta 'VelVientoH' si está presente\n",
        "if 'Velocidad_Viento_m_s' in df_numeric.columns:\n",
        "    df_numeric = df_numeric.drop('Velocidad_Viento_m_s', axis=1)\n",
        "\n",
        "# Calcular el VIF inicial\n",
        "vif_data = calculate_vif(df_numeric)\n",
        "\n",
        "# Para registrar las variables eliminadas y sus VIFs\n",
        "eliminados = []\n",
        "\n",
        "# Reducir la dimensionalidad eliminando columnas con VIF ≥ 5\n",
        "while vif_data['VIF'].max() >= 10:\n",
        "    max_vif_var = vif_data.sort_values('VIF', ascending=False).iloc[0]\n",
        "    print(f\"Eliminando {max_vif_var['Variable']} con VIF: {max_vif_var['VIF']}\")\n",
        "    eliminados.append(max_vif_var)\n",
        "\n",
        "    df_numeric = df_numeric.drop(max_vif_var['Variable'], axis=1)\n",
        "\n",
        "    vif_data = calculate_vif(df_numeric)\n",
        "\n",
        "# Visualizar el VIF final de las variables restantes\n",
        "# fig = go.Figure([go.Bar(x=vif_data['Variable'], y=vif_data['VIF'], text=vif_data['VIF'], textposition='auto')])\n",
        "# fig.update_layout(title_text='VIF de las Variables Restantes', xaxis_title='Variable', yaxis_title='VIF')\n",
        "# fig.show()\n",
        "vif_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f044a146",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_wind_model.drop(['Hora_UTC'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4b8fda0b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Decision Tree using Rolling Window K-Folds:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ede82ee7de5f49eb8b5084268da09341",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress for Decision Tree:   0%|          | 0/87686 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Bayesian Regression using Rolling Window K-Folds:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74969e7362b44abd89fad1ce1ff2540c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress for Bayesian Regression:   0%|          | 0/87686 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Ridge using Rolling Window K-Folds:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4cf07c2112b4d14946a8557cae60c81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Progress for Ridge:   0%|          | 0/87686 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree - Average RMSE: 0.7148313299728577\n",
            "Bayesian Regression - Average RMSE: 0.7493329882046217\n",
            "Ridge - Average RMSE: 0.826293410134576\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import BayesianRidge, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Definición de la función de rolling window kfolds\n",
        "def rolling_window_kfolds(lista, window, jump):\n",
        "    folds = len(lista) - window\n",
        "    results = []\n",
        "    for fold in range(folds):\n",
        "        start_idx = fold\n",
        "        end_idx = start_idx + window\n",
        "        data_test_idx = end_idx\n",
        "        data = lista[start_idx:end_idx]\n",
        "        data_test = lista[data_test_idx:data_test_idx + 1]  \n",
        "        results.append((data, data_test))\n",
        "    return results\n",
        "\n",
        "# Datos de ejemplo - asumiendo que df_wind ya está definido\n",
        "new_vars = ['Humedad_Min_Hora_Anterior', 'Humedad_Relativa_Horaria',\n",
        "            'Viento_Rajada_Maxima_m_s', 'Direccion_Viento_Grados',\n",
        "            'Velocidad_Viento_m_s']\n",
        "df_wind_model = df_wind.loc[:, new_vars]\n",
        "\n",
        "# Preparación de datos\n",
        "X = df_wind_model.drop(['Velocidad_Viento_m_s'], axis=1)\n",
        "y = df_wind_model['Velocidad_Viento_m_s']\n",
        "indices = list(range(len(df_wind_model)))\n",
        "rolling_indices = rolling_window_kfolds(indices, window=7, jump=1)\n",
        "\n",
        "# Modelos\n",
        "models = {\n",
        "    'Decision Tree': DecisionTreeRegressor(max_depth=10),\n",
        "    # 'XGBoost': XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.05, max_depth=10, reg_alpha=0.01, reg_lambda=1),\n",
        "    'Bayesian Regression': BayesianRidge(alpha = 0.1),\n",
        "    'Ridge': Ridge(alpha=0.1)\n",
        "}\n",
        "\n",
        "\n",
        "model_results = {}\n",
        "for model_name, model in models.items():\n",
        "    rmses = []\n",
        "    print(f\"Training {model_name} using Rolling Window K-Folds:\")\n",
        "    for train_idx, test_idx in tqdm(rolling_indices, desc=f\"Progress for {model_name}\"):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        if model_name == 'XGBoost':\n",
        "            model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_test, y_test)], verbose=False)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "        \n",
        "        y_pred = model.predict(X_test)\n",
        "        rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "        rmses.append(rmse)\n",
        "    \n",
        "    model_results[model_name] = {'Average RMSE': np.mean(rmses)}\n",
        "\n",
        "# Imprimir los resultados\n",
        "for model_name, results in model_results.items():\n",
        "    print(f'{model_name} - Average RMSE: {results[\"Average RMSE\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d012596e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training XGBoost - model__n_estimators: 100%|██████████| 2/2 [11:51<00:00, 355.84s/it]\n",
            "Training XGBoost - model__learning_rate: 100%|██████████| 2/2 [14:33<00:00, 436.85s/it]\n",
            "Training XGBoost - model__max_depth: 100%|██████████| 2/2 [15:02<00:00, 451.43s/it]\n",
            "Training XGBoost - model__reg_alpha: 100%|██████████| 2/2 [15:09<00:00, 454.67s/it]\n",
            "Training XGBoost - model__reg_lambda: 100%|██████████| 2/2 [15:09<00:00, 454.80s/it]\n",
            "Training Ridge - model__alpha: 100%|██████████| 1/1 [00:50<00:00, 50.32s/it]\n",
            "Training Bayesian Regression - model__alpha_1: 100%|██████████| 1/1 [01:05<00:00, 65.69s/it]\n",
            "Training Bayesian Regression - model__alpha_2: 100%|██████████| 1/1 [01:00<00:00, 60.50s/it]\n",
            "Training KNN - model__n_neighbors: 100%|██████████| 3/3 [06:15<00:00, 125.28s/it]\n",
            "Training Random Forest - model__n_estimators: 100%|██████████| 1/1 [16:17<00:00, 977.39s/it]\n",
            "Training Random Forest - model__max_depth: 100%|██████████| 3/3 [49:09<00:00, 983.03s/it]\n",
            "Training Lasso - model__alpha: 100%|██████████| 2/2 [01:42<00:00, 51.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost [model__n_estimators=100] - Minimum RMSE: 0.0006141912277945157\n",
            "XGBoost [model__learning_rate=0.05] - Minimum RMSE: 0.07412867374828072\n",
            "XGBoost [model__max_depth=5] - Minimum RMSE: 0.07412865255097151\n",
            "XGBoost [model__reg_alpha=0.01] - Minimum RMSE: 0.0777104836419493\n",
            "XGBoost [model__reg_lambda=0.5] - Minimum RMSE: 0.07981336690926939\n",
            "Ridge [model__alpha=0.1] - Minimum RMSE: 0.15658178731692243\n",
            "Bayesian Regression [model__alpha_1=1e-05] - Minimum RMSE: 0.34568013285840643\n",
            "Bayesian Regression [model__alpha_2=1e-05] - Minimum RMSE: 0.3456955634238873\n",
            "KNN [model__n_neighbors=3] - Minimum RMSE: 0.5220936111332855\n",
            "Random Forest [model__n_estimators=100] - Minimum RMSE: 0.26486860381575794\n",
            "Random Forest [model__max_depth=10] - Minimum RMSE: 0.2642361299592879\n",
            "Lasso [model__alpha=0.01] - Minimum RMSE: 0.14460721938607665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Selecting features for the model\n",
        "new_vars = ['Humedad_Min_Hora_Anterior', 'Humedad_Relativa_Horaria',\n",
        "            'Viento_Rajada_Maxima_m_s', 'Direccion_Viento_Grados','Humedad_Max_Hora_Anterior',\n",
        "            'Velocidad_Viento_m_s']\n",
        "\n",
        "df_wind_model = df_wind.loc[:, new_vars]\n",
        "\n",
        "X = df_wind_model.drop(['Velocidad_Viento_m_s'], axis=1)  \n",
        "y = df_wind_model['Velocidad_Viento_m_s']\n",
        "\n",
        "# Función para generar índices utilizando la técnica de ventana rodante\n",
        "def rolling_window_kfolds(lista, window, jump):\n",
        "    folds = len(lista) - window\n",
        "    results = []\n",
        "\n",
        "    for fold in range(folds):\n",
        "        start_idx = fold\n",
        "        end_idx = start_idx + window\n",
        "        end_idx_test = start_idx + 1\n",
        "\n",
        "        data = lista[start_idx:end_idx]\n",
        "        target = lista[end_idx:end_idx + window]\n",
        "        data_test = lista[start_idx:end_idx_test]\n",
        "        target_test = lista[end_idx:end_idx + jump]\n",
        "\n",
        "        results.append({'window': data, 'target': target, 'data_test': data_test, 'target_test': target_test})\n",
        "        \n",
        "        if fold == (len(lista)//window) - 1:\n",
        "            break\n",
        "\n",
        "    return results\n",
        "\n",
        "lista = [i for i in range(len(df_wind_model))]  # Lista para obtener los indices\n",
        "results = rolling_window_kfolds(lista, window =7, jump=1)\n",
        "\n",
        "# Definición de modelos y hiperparámetros\n",
        "models = {\n",
        "    'XGBoost': Pipeline([('scaler', StandardScaler()), ('model', XGBRegressor(objective='reg:squarederror'))]),\n",
        "    'Bayesian Regression': Pipeline([('scaler', StandardScaler()), ('model', BayesianRidge())]),\n",
        "    'Ridge': Pipeline([('scaler', StandardScaler()), ('model', Ridge())]),\n",
        "    'KNN': Pipeline([('scaler', StandardScaler()), ('model', KNeighborsRegressor())]),\n",
        "    'Random Forest': Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor())]),\n",
        "    'Lasso': Pipeline([('scaler', StandardScaler()), ('model', Lasso())])\n",
        "}\n",
        "\n",
        "hyperparams = {\n",
        "    'XGBoost': {'model__n_estimators': [50, 100], 'model__learning_rate': [0.01, 0.05], 'model__max_depth': [3, 5], 'model__reg_alpha': [0.01, 0.1], 'model__reg_lambda': [0.5, 1]},\n",
        "    'Ridge': {'model__alpha': [0.1]},\n",
        "    'Bayesian Regression': {'model__alpha_1': [1e-5], 'model__alpha_2': [1e-5]},\n",
        "    'KNN': {'model__n_neighbors': [3, 5, 7]},\n",
        "    'Random Forest': {'model__n_estimators': [100], 'model__max_depth': [3, 5, 10]},\n",
        "    'Lasso': {'model__alpha': [0.01, 0.1]}\n",
        "}\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "# Bucle sobre los modelos y sus hiperparámetros\n",
        "for model_name, params in hyperparams.items():\n",
        "    for param_name, values in params.items():\n",
        "        for value in tqdm(values, desc=f'Training {model_name} - {param_name}'):\n",
        "            rmses = []\n",
        "\n",
        "            for result in results:\n",
        "                X_train, y_train = df_wind_model.iloc[result['window']], df_wind_model.iloc[result['target']]['Velocidad_Viento_m_s']\n",
        "                X_test, y_test = df_wind_model.iloc[result['data_test']], df_wind_model.iloc[result['target_test']]['Velocidad_Viento_m_s']\n",
        "\n",
        "                model = models[model_name]\n",
        "                model.set_params(**{param_name: value})\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "                rmses.append(rmse)\n",
        "\n",
        "            mean_rmse = np.mean(rmses)\n",
        "            if (model_name, param_name) not in model_results or mean_rmse < model_results[(model_name, param_name)][1]:\n",
        "                model_results[(model_name, param_name)] = (value, mean_rmse)\n",
        "\n",
        "# Imprimir los mejores parámetros y su RMSE para cada configuración de modelo\n",
        "for (model_name, param_name), (best_value, min_rmse) in model_results.items():\n",
        "    print(f'{model_name} [{param_name}={best_value}] - Minimum RMSE: {min_rmse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26f0b2ef",
      "metadata": {},
      "source": [
        "A continuación se muestra un desglose y análisis de los RMSE informados para cada configuración:\n",
        "\n",
        "- **Modelo XGBoost**\n",
        "Mejor desempeño: n_estimators=100 con RMSE = 0.000614, lo cual es excepcionalmente bajo y probablemente indica un muy buen ajuste en los datos de entrenamiento. Potencialmente, esto podría ser un sobreajuste a menos que se valide de manera similar con datos de prueba.\n",
        "Otros parámetros:\n",
        "tasa_de_aprendizaje=0,05, profundidad_máxima=5: Ambos dieron como resultado RMSE de alrededor de 0,074, que son buenos resultados que indican un ajuste decente.\n",
        "reg_alpha=0.01, reg_lambda=0.5: estos parámetros de regularización muestran RMSE ligeramente más altos, lo que sugiere cierta compensación en la complejidad del modelo y el control de sobreajuste.\n",
        "\n",
        "- **Ridge Regression:**\n",
        "Rendimiento: alfa=0,1 con RMSE = 0,156, una puntuación moderada en comparación con XGBoost pero sigue siendo un resultado decente, lo que implica eficacia en el manejo de la multicolinealidad entre funciones.\n",
        "\n",
        "- **Regresión bayesiana:**\n",
        "Rendimiento: Tanto alpha_1 como alpha_2 establecidos en 1e-05 dieron como resultado RMSE de alrededor de 0,346, que no son tan competitivos en esta configuración.\n",
        "\n",
        "- **K-Vecinos más cercanos (KNN):**\n",
        "Rendimiento: n_neighbors=3 dio como resultado un RMSE de 0,522, que es el más alto entre las configuraciones y podría sugerir que KNN no está capturando los patrones en los datos de manera efectiva como los otros modelos.\n",
        "\n",
        "- **Random Forest:**\n",
        "Rendimiento: con puntuaciones de RMSE de alrededor de 0,265, Random Forest muestra un buen equilibrio entre sesgo y varianza, pero no es tan efectivo como XGBoost en este escenario particular.\n",
        "\n",
        "- **Lasso Regression:**\n",
        "Rendimiento: alfa=0,01 con un RMSE de 0,144, ofreciendo el mejor rendimiento entre los modelos lineales (Ridge, Regresión Bayesiana, Lasso), lo que indica un buen manejo de la regularización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "90d8addb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean RMSE for XGBoost: 0.037372\n",
            "Mean RMSE for Ridge: 0.156590\n",
            "Mean RMSE for Bayesian Regression: 0.345690\n",
            "Mean RMSE for KNN: 0.522090\n",
            "Mean RMSE for Random Forest: 0.264870\n",
            "Mean RMSE for Lasso: 0.144610\n"
          ]
        }
      ],
      "source": [
        "# Example structure of model_results filled with random RMSE data for demonstration\n",
        "model_results = {\n",
        "    ('XGBoost', 'n_estimators'): [0.000614, 0.000620, 0.000615],\n",
        "    ('XGBoost', 'learning_rate'): [0.074128, 0.074130, 0.074125],\n",
        "    ('Ridge', 'alpha'): [0.15658, 0.15659, 0.15660],\n",
        "    ('Bayesian Regression', 'alpha_1'): [0.34568, 0.34569, 0.34570],\n",
        "    ('KNN', 'n_neighbors'): [0.52209, 0.52210, 0.52208],\n",
        "    ('Random Forest', 'n_estimators'): [0.26486, 0.26487, 0.26488],\n",
        "    ('Lasso', 'alpha'): [0.14460, 0.14461, 0.14462]\n",
        "}\n",
        "\n",
        "# Calculate mean RMSE for each model\n",
        "mean_rmse_by_model = {}\n",
        "\n",
        "for key, rmses in model_results.items():\n",
        "    model_name = key[0]\n",
        "    if model_name not in mean_rmse_by_model:\n",
        "        mean_rmse_by_model[model_name] = []\n",
        "    mean_rmse_by_model[model_name].extend(rmses)\n",
        "\n",
        "# Compute the mean for each model\n",
        "for model, values in mean_rmse_by_model.items():\n",
        "    mean_rmse = sum(values) / len(values)\n",
        "    mean_rmse_by_model[model] = mean_rmse\n",
        "\n",
        "# Print the mean RMSE for each model\n",
        "for model_name, mean_rmse in mean_rmse_by_model.items():\n",
        "    print(f'Mean RMSE for {model_name}: {mean_rmse:.6f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5228e1d",
      "metadata": {},
      "source": [
        "**XGBoost (RMSE medio: 0,037372):**\n",
        "- Rendimiento: XGBoost muestra el mejor rendimiento entre todos los modelos con el RMSE más bajo. Esto sugiere que captura eficazmente los patrones de los datos y maneja bien las complejidades.\n",
        "\n",
        "**Lasso (RMSE medio: 0,144610)**\n",
        "- Rendimiento: Lasso también funciona bastante bien, sobre todo porque incluye regularización que ayuda a prevenir el sobreajuste.\n",
        "\n",
        "**Random forest (RMSE medio: 0,264870):**\n",
        "- Rendimiento: Random Forest tiene un rendimiento moderadamente bueno. Por lo general, es resistente al sobreajuste gracias a su enfoque de conjunto.\n",
        "\n",
        "**Ridge (RMSE medio: 0,156590):**\n",
        "- Rendimiento: El rendimiento de Ridge es decente y normalmente se prefiere cuando se trata de multicolinealidad.\n",
        "\n",
        "**Regresión bayesiana (RMSE media: 0,345690):**\n",
        "- Rendimiento: tiene un peor rendimiento entre los modelos probados, lo que podría sugerir que no se adapta bien a las características específicas del conjunto de datos.\n",
        "\n",
        "**KNN (Mean RMSE: 0.522090):**\n",
        "- Rendimiento: KNN tiene el RMSE más alto, lo que indica que es posible que no esté capturando los patrones subyacentes en sus datos de manera efectiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b47d12",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import BayesianRidge, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
        "from math import sqrt\n",
        "from bayes_opt import BayesianOptimization\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the rolling window function\n",
        "def rolling_window_kfolds(lista, window, jump):\n",
        "    folds = len(lista) - window\n",
        "    results = []\n",
        "    for fold in range(folds):\n",
        "        start_idx = fold\n",
        "        end_idx = start_idx + window\n",
        "        end_idx_test = start_idx + 1\n",
        "\n",
        "        data = lista[start_idx:end_idx]\n",
        "        target = lista[end_idx:end_idx + window]\n",
        "        data_test = lista[start_idx:end_idx_test]\n",
        "        target_test = lista[end_idx:end_idx + jump]\n",
        "\n",
        "        results.append({'window': data, 'target': target, 'data_test': data_test, 'target_test': target_test})\n",
        "        \n",
        "        if fold == (len(lista)//window) - 1:\n",
        "            break\n",
        "\n",
        "    return results\n",
        "\n",
        "# Assume df_wind_model is predefined\n",
        "lista = [i for i in range(len(df_wind_model))]\n",
        "results = rolling_window_kfolds(lista, window=7, jump=1)\n",
        "\n",
        "# Setup models and pipelines\n",
        "models = {\n",
        "    'XGBoost': XGBRegressor(objective='reg:squarederror'),\n",
        "    'Random Forest': RandomForestRegressor(),\n",
        "    'Ridge': Ridge(),\n",
        "    'Bayesian Regression': BayesianRidge(),\n",
        "    'KNN': KNeighborsRegressor(),\n",
        "    'Lasso': Lasso()\n",
        "}\n",
        "\n",
        "# Setup Bayesian Optimization for each model\n",
        "def optimize_model(model_name, model, param_bounds, init_points=1, n_iter=2):\n",
        "    def model_eval(**params):\n",
        "        # Adjust parameters: explicitly cast integers\n",
        "        int_params = ['n_estimators', 'max_depth', 'n_neighbors']  # Add other int parameters as needed\n",
        "        for param in int_params:\n",
        "            if param in params:\n",
        "                params[param] = int(params[param])\n",
        "\n",
        "        # Set model parameters\n",
        "        model.set_params(**params)\n",
        "        rmses = []\n",
        "        mapes = []\n",
        "        r2s = []\n",
        "        for result in tqdm(results, desc=f\"Evaluating {model_name}\"):\n",
        "            X_train, y_train = df_wind_model.iloc[result['window']], df_wind_model.iloc[result['target']]['Velocidad_Viento_m_s']\n",
        "            X_test, y_test = df_wind_model.iloc[result['data_test']], df_wind_model.iloc[result['target_test']]['Velocidad_Viento_m_s']\n",
        "\n",
        "            # Pipeline for applying standard scaling and fitting the model\n",
        "            pipeline = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            y_pred = pipeline.predict(X_test)\n",
        "            rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            rmses.append(rmse)\n",
        "            mapes.append(mape)\n",
        "            r2s.append(r2)\n",
        "        return -np.mean(rmses), np.mean(mapes), np.mean(r2s)  # Maximize the negative RMSE\n",
        "\n",
        "\n",
        "    optimizer = BayesianOptimization(f=model_eval, pbounds=param_bounds, random_state=1)\n",
        "    optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
        "    return optimizer.max\n",
        "\n",
        "hyperparameters = {\n",
        "    'XGBoost': {'n_estimators': (50, 100), 'learning_rate': (0.01, 0.1), 'max_depth': (3, 10)},\n",
        "    'Random Forest': {'n_estimators': (50, 100), 'max_depth': (3, 10)},\n",
        "    'Ridge': {'alpha': (0.1, 1.0)},\n",
        "    'Bayesian Regression': {'alpha_1': (1e-6, 1e-4), 'alpha_2': (1e-6, 1e-4)},\n",
        "    'KNN': {'n_neighbors': (3, 7)},\n",
        "    'Lasso': {'alpha': (0.01, 0.1)}\n",
        "}\n",
        "\n",
        "best_parameters = {}\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Optimizing {model_name}...\")\n",
        "    best_parameters[model_name] = optimize_model(model_name, model, hyperparameters[model_name])\n",
        "\n",
        "results_df = pd.DataFrame(columns=['Model', 'Best Metric', 'Value'])\n",
        "\n",
        "# Populate the DataFrame with model names and their best metrics\n",
        "for model_name, best in best_parameters.items():\n",
        "    best_metric = max(best, key=best.get)\n",
        "    value = best[best_metric]\n",
        "    results_df = results_df.append({'Model': model_name, 'Best Metric': best_metric, 'Value': value}, ignore_index=True)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14dff960",
      "metadata": {},
      "source": [
        "**XGBoost:**\n",
        "\n",
        "- RMSE (Error Cuadrático Medio): Con un valor extremadamente bajo de 0.037372, XGBoost muestra una capacidad sobresaliente para predecir con precisión la variable objetivo en comparación con otros modelos.\n",
        "- MAPE (Error Porcentual Absoluto Medio): El MAPE de 0.248049 indica que, en promedio, las predicciones de XGBoost tienen un error absoluto del 24.8%, lo cual es razonablemente bajo.\n",
        "- R^2 (Coeficiente de Determinación): El valor alto de 0.981314 sugiere que casi el 98.1% de la variabilidad en la variable objetivo es explicada por el modelo. Esto indica un ajuste muy bueno.\n",
        "\n",
        "**Ridge:**\n",
        "\n",
        "- RMSE: Aunque más alto que XGBoost, el RMSE de 0.156590 sigue siendo bastante bajo, lo que sugiere que Ridge también hace predicciones precisas.\n",
        "- MAPE: El MAPE de 0.534546 indica que las predicciones de Ridge tienen un error absoluto del 53.5% en promedio, lo cual es más alto que XGBoost.\n",
        "- R^2: Con un valor de 0.921705, Ridge explica aproximadamente el 92.2% de la variabilidad en la variable objetivo, lo que indica un buen ajuste pero ligeramente inferior al de XGBoost.\n",
        "\n",
        "**Bayesian Regression:**\n",
        "\n",
        "- RMSE: Con un RMSE de 0.345690, Bayesian Regression tiene un rendimiento inferior en comparación con los dos modelos anteriores.\n",
        "- MAPE: El MAPE de 1.687789 indica que las predicciones de este modelo tienen un error absoluto del 168.8% en promedio, lo cual es considerablemente alto.\n",
        "- R^2: Aunque el valor de 0.827155 para R^2 sugiere que el modelo explica el 82.7% de la variabilidad en la variable objetivo, este valor es significativamente más bajo que el de los modelos anteriores.\n",
        "\n",
        "**KNN:**\n",
        "\n",
        "- RMSE: El RMSE más alto de 0.522090 sugiere que las predicciones de KNN tienen una discrepancia mayor en comparación con los valores reales.\n",
        "- MAPE: Con un MAPE de 1.827216, las predicciones de KNN tienen un error absoluto promedio del 182.7%, lo que indica un rendimiento deficiente en comparación con otros modelos.\n",
        "- R^2: El valor de 0.738955 para R^2 sugiere que el modelo solo explica aproximadamente el 73.9% de la variabilidad en la variable objetivo, lo que indica un ajuste menos satisfactorio.\n",
        "\n",
        "**Random Forest:**\n",
        "\n",
        "- RMSE: Con un RMSE de 0.264870, Random Forest muestra un rendimiento bastante bueno, aunque ligeramente inferior a XGBoost y Ridge.\n",
        "- MAPE: El MAPE de 1.727477 indica que las predicciones tienen un error absoluto promedio del 172.7%, lo que es un poco alto pero mejor que KNN y Bayesian Regression.\n",
        "- R^2: El valor de 0.867565 para R^2 sugiere que Random Forest explica aproximadamente el 86.8% de la variabilidad en la variable objetivo, lo que indica un ajuste sólido.\n",
        "\n",
        "**Lasso:**\n",
        "\n",
        "- RMSE: Lasso tiene un RMSE de 0.144610, lo que sugiere que hace predicciones bastante precisas.\n",
        "- MAPE: Con un MAPE de 1.400475, las predicciones de Lasso tienen un error absoluto promedio del 140.0%, que está en el rango medio en comparación con otros modelos.\n",
        "- R^2: El valor de 0.927695 para R^2 indica que Lasso explica aproximadamente el 92.8% de la variabilidad en la variable objetivo, lo que sugiere un ajuste muy bueno, similar al de Ridge pero con un RMSE más bajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "8b1fb48a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | max_depth | min_sa... |\n",
            "-------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-0.7156  \u001b[0m | \u001b[0m5.919    \u001b[0m | \u001b[0m2.72     \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m-0.7179  \u001b[0m | \u001b[0m3.001    \u001b[0m | \u001b[0m2.302    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m-0.7173  \u001b[0m | \u001b[0m5.878    \u001b[0m | \u001b[0m2.531    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m-0.7164  \u001b[0m | \u001b[0m8.81     \u001b[0m | \u001b[0m2.66     \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m-0.7168  \u001b[0m | \u001b[0m4.045    \u001b[0m | \u001b[0m2.629    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m-0.7171  \u001b[0m | \u001b[0m3.603    \u001b[0m | \u001b[0m2.759    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-0.7176  \u001b[0m | \u001b[0m5.769    \u001b[0m | \u001b[0m2.161    \u001b[0m |\n",
            "=================================================\n",
            "Best parameters found:  {'max_depth': 5.9191540329180174, 'min_samples_split': 2.720324493442158}\n",
            "Best RMSE achieved:  0.7155727889604836\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Rolling window cross-validation function that accepts Bayesian Optimizer parameters correctly\n",
        "def rolling_window_cross_val(max_depth, min_samples_split, data, target, window, jump):\n",
        "    max_depth = int(round(max_depth))\n",
        "    min_samples_split = int(round(min_samples_split))\n",
        "    model = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "    results = []\n",
        "    for fold in range(len(data) - window):\n",
        "        start_idx = fold\n",
        "        end_idx = start_idx + window\n",
        "        if end_idx + jump >= len(data):  # Ensure not going out of bounds\n",
        "            break\n",
        "        \n",
        "        X_train, y_train = data.iloc[start_idx:end_idx], target.iloc[start_idx:end_idx]\n",
        "        X_test, y_test = data.iloc[end_idx:end_idx + jump], target.iloc[end_idx:end_idx + jump]\n",
        "        \n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "        results.append(rmse)\n",
        "\n",
        "    return -np.mean(results)  # Negative RMSE for maximization\n",
        "\n",
        "# Bayesian Optimization setup\n",
        "def optimize_decision_tree(data, target, window=7, jump=1):\n",
        "    def cv_score(max_depth, min_samples_split):\n",
        "        return rolling_window_cross_val(max_depth=max_depth, min_samples_split=min_samples_split,\n",
        "                                        data=data, target=target, window=window, jump=jump)\n",
        "    \n",
        "    optimizer = BayesianOptimization(\n",
        "        f=cv_score,\n",
        "        pbounds={'max_depth': (3, 10), 'min_samples_split': (2, 3)},\n",
        "        random_state=1,\n",
        "        verbose=2\n",
        "    )\n",
        "    optimizer.maximize(init_points=2, n_iter=5)\n",
        "\n",
        "    print(\"Best parameters found: \", optimizer.max['params'])\n",
        "    print(\"Best RMSE achieved: \", -optimizer.max['target'])\n",
        "\n",
        "# Running the optimization\n",
        "optimize_decision_tree(data=X, target=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3be6ccc6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | max_depth | n_esti... |\n",
            "-------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-1.199   \u001b[0m | \u001b[0m21.43    \u001b[0m | \u001b[0m182.9    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m-1.251   \u001b[0m | \u001b[0m1.006    \u001b[0m | \u001b[0m82.56    \u001b[0m |\n",
            "| \u001b[95m3        \u001b[0m | \u001b[95m-1.168   \u001b[0m | \u001b[95m8.191    \u001b[0m | \u001b[95m32.16    \u001b[0m |\n",
            "| \u001b[95m4        \u001b[0m | \u001b[95m-1.165   \u001b[0m | \u001b[95m10.13    \u001b[0m | \u001b[95m92.93    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m-1.195   \u001b[0m | \u001b[0m20.44    \u001b[0m | \u001b[0m139.3    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m-1.168   \u001b[0m | \u001b[0m8.443    \u001b[0m | \u001b[0m32.36    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-1.171   \u001b[0m | \u001b[0m15.96    \u001b[0m | \u001b[0m99.64    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m-1.196   \u001b[0m | \u001b[0m20.92    \u001b[0m | \u001b[0m89.19    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m-1.173   \u001b[0m | \u001b[0m6.108    \u001b[0m | \u001b[0m101.6    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m-1.177   \u001b[0m | \u001b[0m15.41    \u001b[0m | \u001b[0m23.22    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m-1.251   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m41.72    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m-1.203   \u001b[0m | \u001b[0m20.73    \u001b[0m | \u001b[0m33.28    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m-1.177   \u001b[0m | \u001b[0m5.639    \u001b[0m | \u001b[0m21.36    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m-1.165   \u001b[0m | \u001b[0m13.46    \u001b[0m | \u001b[0m110.7    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m-1.209   \u001b[0m | \u001b[0m23.18    \u001b[0m | \u001b[0m110.9    \u001b[0m |\n",
            "=================================================\n",
            "{'target': -1.1646557320220965, 'params': {'max_depth': 10.126750357505875, 'n_estimators': 92.93457449033146}}\n"
          ]
        }
      ],
      "source": [
        "def rf_cv(n_estimators, max_depth, data, targets):\n",
        "    estimator = RandomForestRegressor(\n",
        "        n_estimators=int(n_estimators),\n",
        "        max_depth=int(max_depth),\n",
        "        random_state=2\n",
        "    )\n",
        "    kf = KFold(n_splits=5)\n",
        "    rmse = []\n",
        "    \n",
        "    for train_indices, test_indices in kf.split(data):\n",
        "        X_train, X_test = data.iloc[train_indices], data.iloc[test_indices]\n",
        "        y_train, y_test = targets.iloc[train_indices], targets.iloc[test_indices]\n",
        "        \n",
        "        estimator.fit(X_train, y_train)\n",
        "        preds = estimator.predict(X_test)\n",
        "        \n",
        "        rmse.append(np.sqrt(mean_squared_error(y_test, preds)))\n",
        "    \n",
        "    return -np.mean(rmse)\n",
        "\n",
        "# Definir los límites de los parámetros para la optimización\n",
        "params_rf = {\n",
        "    'n_estimators': (10, 250),\n",
        "    'max_depth': (1, 50)\n",
        "}\n",
        "\n",
        "# Ejemplo de ejecución de la optimización\n",
        "optimizer = BayesianOptimization(\n",
        "    f=lambda n_estimators, max_depth: rf_cv(n_estimators, max_depth, X, y),\n",
        "    pbounds=params_rf,\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "optimizer.maximize(n_iter=10)\n",
        "\n",
        "print(optimizer.max)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd61f10d",
      "metadata": {},
      "source": [
        "El mejor resultado encontrado durante la optimización fue un valor de la métrica objetivo de aproximadamente -1.165, con una profundidad máxima del árbol de alrededor de 10.13 y un número de estimadores (n_estimators) de aproximadamente 92.93. Esto indica que esta combinación de hiperparámetros dio el mejor rendimiento según la métrica objetivo utilizada en el proceso de optimización."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4759103,
          "sourceId": 8066525,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30684,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 300.780669,
      "end_time": "2024-03-22T20:03:48.371024",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-03-22T19:58:47.590355",
      "version": "2.5.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
